% \SweaveOpts{prefix.string=../sweavegraphic}
\documentclass[fleqn,10pt]{./paper_sr_format/wlscirep}
\usepackage{float}
\usepackage[utf8]{inputenc}
% \setcounter{secnumdepth}{10}
%\setcounter{tocdepth}{10}
%\usepackage[nomarkers,figuresonly]{float}

\title{Spatial Multiscale entropy}

\author[1,*]{Martin Barner}
\author[2]{Elsa Arcaute}
\affil[1]{funemployed}
\affil[2]{Centre for Advanced Spatial Analysis, London, postcode, country}

\affil[*]{barner.mar@gmail.com}

% \affil[+]{these authors contributed equally to this work}

\keywords{entropy, spatial, cities, complexity, urban}

\begin{abstract}
We propose a multiscale measure of entropy for spatial configurations in the context of cities. Existing approaches usually use entropy as a proxy for spatial evenness or disorder, and translate the phase space from statistical mechanics literally to the geographical space. In contrast, we focus on building an interpretation of entropy that is conceptually consistent with statistical mechanics as well as the view of cities as complex systems. For this, we focus on the characteristics of places and, most importantly, take into account interactions between them.
By comparing synthetic patterns, we show that if elements of a system interact spatially, complex spatial patterns display higher entropy. We can thus partly explain morphological complexity in cities as simply the most probable configuration. Further, we conduct a case study of entropy in the spatial distribution of buildings with different functions in West London over a period of 130 years. We find that the polycentric sprawl increases entropy compared to spatially random or segregated patterns. This work offers a new approach to explaining how morphological complexity in cities emerges from individual behaviour. Furthermore, it may be helpful in dealing with uncertainty in planning.
\end{abstract}
\begin{document}

\flushbottom
\maketitle
% * <john.hammersley@gmail.com> 2015-02-09T12:07:31.197Z:
%
%  Click the title above to edit the author information and abstract
%
% \tableofcontents

\thispagestyle{empty}
\setlength\parindent{0pt}
\chapter{Test}
\section*{Introduction}
% why entropy?
Entropy in thermodynamics is a concept relating the fast, microscopic behaviour to the slow, macroscopic dynamics of a system. We therefore see it as a suitable tool to be used in studying the relationship between the fast dynamics of individual behaviour and the slow, larger scale dynamics of change in urban structures. Batty recognises not only a ``literature gap'', but ``an entirely new research agenda'' for spatial entropy measures for cities and states that ``substantive interpretations of entropy measures [\ldots] have not been well developed. [\ldots] I sketch the need for new and different entropy measures that enable us to see how equilibrium spatial distributions can be generated as the outcomes of dynamic processes''\cite{Batty2010}. \\ 

% what is entropy?
In its essence, entropy in statistical mechanics is concerned with the ``number of ways [\ldots] by which a given macroscopic state can be realized''\cite{LiveEarthEntropy}.
If more combinations of the possible microstates of the elements in a system constitue the same macroscopic state of that system, that macroscopic state has higher entropy and is more likely to occur. The space of possible microstates of elements is called the phase space. The more evenly elements are distributed across the phase space, the greater the entropy.
There are for example more ways to distribute molecules evenly in a room than to place them all in one corner. Molecules do float around randomly in space and randomly exchange energy, and so in a phase space with dimensions describing their position and momentum, they are most likely to be distributed as evenly as possible.\cite{GeneralPropertiesOfEntropy}\\

% phase space in cities?
Buildings do not float around in space randomly, and they do not have a momentum. To define a relevant phase space for urban morphology, we must carefully consider the processes that produce the spatial patterns in cities. 

% existing measures
Existing measures of entropy in an urban context are either non-spatial, or use the geographical space directly as the phase space. Entropy then becomes a proxy for spatial evenness. While on the surface this phase space is very similar to thermodynamics, it is not necessarily representative of how the macroscopic state of a city evolves.
%multiscale entropy: differences
In this paper, we attempt to formulate a measure of entropy that is conceptually consistent with entropy in statistical mechanics and an understanding of cities as complex systems. Two fundamental aspects thus differentiate our approach from existing spatial measures of entropy:\\

%1. complex systems:  Characteristics
First, the phase space dimensions reflect characteristics of places instead of absolute locations, because cities are complex systems: 
Viewing cities as complex systems brought a fundamental shift in our understanding of how cities function, grow, and change over time\cite{Jane,NewScience,CitiesAndComplexity,allen1997cities}. Essential to this view is that a city ``self-organizes out of millions of individual decisions, a global order built out of local interactions''\cite[p.38]{JohnsonEmergence}. What people do in the city has an impact on its spatial structure over long periods of time \cite{Portugali1,AllenCoevolution}.
It follows that in terms of state probabilites, a measure of morphological entropy should relate to how the city is used. 
We are not interested in the randomness of geographic coordinates, but in the randomness of how people could use the city depending on its physical structure. Our phase space dimensions describe the characteristics of different places, based on the assumption that what people can do in two places differs if the places have different characteristics. If there are more combinations of possible use for a city's morphological macrostate, there are also more ways that macrostate can be realised.
Instead of measuring spatial evenness - the uncertainty about where things are - we attempt to measure the uncertainty about urban life that is built into the spatial structure of the city.\\

% 2. Interactions.
Second, the phase space dimensions reflect that places in a city are inherently dependent on each other.
Boltzman's entropy assumes that interactions between particles are neglectable. Existing measures of entropy inherit this for cities, and assume that there were no interactions between places. We recognise that different places are highly dependent on each other due to flows and interactions between the people in them. This interdependence between places is widely recognised in geography in general\cite{Tobler1970}, urban theory\cite{Jane,NotATree} and quantitatively demonstrated for example in the success of spatial interaction models\cite{WilsonFamily,SpatialInteraction1,GisModels,WilsonSIMretail}, and further in the study of agglomeration economics \cite{agglomerationEconomics} and neighbourhood effects \cite{neighbourhoodeffects}.
It follows that the states of individual places must be entangled spatially, because in terms of how they are used, places in a city are entangled spatially as well. In that sense the state of an observed place should in some way also incorporate the characteristics of surrounding places.

Our new measure observes the characteristics of places, and takes relationships between places into account with a multiscale approach: We observe the characteristics of a place at multiple scales by aggregating the characteristics of neighbourhoods with different sizes around it. The resulting values then define its position in the phase space. The state of a place is then not only given by its own characteristic, but also the characteristics of the directly adjacent places, its local neighbourhood and larger scale surroundings.\\

%% Multiscale entropy
By no means we ever question that greater randomness and a more even probability distribution in the phase space results in higher entropy.  That is by definition a physical, statistical and mathematical fact. It must be very clear that the phase space here is not equivalent to the geographical space. Randomness in the phase space is not equivalent to randomness in the geographical space. There is no reason to assume that in cities spatially uniform patterns can occur through the largest number of combinations of individual use. There is no reason to assume that the geographical space should be the phase space, nor that spatial randomness should automatically mean randomness in the phase space. This does not in any way violate the fact that greater randomness of microstates and an even distribution in the phase space result in higher entropy.  Our methods and results are perfectly consistent with this.\\

That being said, our analysis of simulated patterns shows that urban structures that are completely randomised spatially do not simultaneously have the highest randomness in a phase space that describes the variety of available types of interdependent places. Instead, we can show that spatially complex patterns are most evenly distributed in the multiscale phase space and have the highest entropy. That they are not evenly distributed in geographical space is in perfect harmony with the fact that greater randomness means greater entropy, because \emph{the geographical space is not the phase space}. 
In our case study we measure the change in multiscale entropy in land use patterns in West London in seven time steps from 1875 to 2005, and the results suggest that the observed polycentric sprawl can be explained as the growth pattern with the highest entropy. 


\section*{A multiscale approach to entropy in cities}
In this chapter we summarise existing research and introduce the multiscale entropy measure in detail. First, we revisit the formal definition of entropy and the phase space in statistical mechanics. We then use them to summarize existing approaches to entropy in cities based on two typical interpretations of the phase space. After this  review of previous research we establish its shortcomings, and recapulate our requirements for a new measure of entropy to observe place's characteristics and take into account interactions. With a combinatorical example we show why a multiscale approach is suitable for this. Finally, we formally define the theoretical multiscale phase space in which each state is given by a matrix containing multiple characteristics aggregated in neighbourhoods of different sizes, and introduce simplifications to achieve a practical method for multiscale entropy estimation.\\

\subsection*{Entropy in Statistical Mechanics}
Entropy is defined as\cite{Shannon}
\begin{equation}
\label{eq:continuous_entropy}
 H = - \int{\! f(x) \log(f(x)) \, \mathrm{d}x}
\end{equation}
where $f(x)$ is the probability density of a continuous phase space. The equivalent to equation \ref{eq:continuous_entropy} in the discrete phase space is \cite{Shannon}:
\begin{equation}
\label{eq:shannon_entropy}
 H = -\sum{p \log{p}}
\end{equation}

which reduces to the Boltzman entropy $S$ if all microstate probabilites $p$ are the same:
\begin{equation}
\label{eq:boltzman_entropy}
 S = k_B \log(\Omega)
\end{equation}

Where $k_B$ is the Boltzmann constant and $\Omega$ the number of accessible microstates\cite[p.44]{BoltzmannOmega}. All microstates can be allocated a location in the phase space. If the phase space is discrete, we can count the number of possible permutations that produce the same macrostate.\\

The highest entropy is always given by a uniform distribution in the phase space, leading to sometimes misleading but common metaphors for entropy \cite{EntropyMetaphors}: if the entropy of a system is high, the state of a randomly selected element is unpredictable and ``uncertain'', and if we are uncertain about where things are in a system, one might describe it as ``disordered''. \\

It is commonly understood in thermodynamics that if one refers to the Boltzman phase space, it usually relates to the six dimensional phase space that defines a particle's state by its location and momentum. Similarily, the Gibbs phase space relates to the 6N dimensional phase space describing the location and momentum of all N particles in the system.\cite{GibbsVSBoltzmann}

\subsection*{Common Phase Space Definitions for Spatial Entropy}
In contrast to thermodynamics where usually there is a general consesus about what the the parameters of a microstate are, there are fundamental differences between interpretations of entropy in cities, and they come down to different definitions of the phase space dimensions.
\begin{itemize}
\item{The first essential approach in the literature takes the word ``space'' literally and defines the phase space as the geographical space.\cite{SpatialEntropyBatty1974,Batty2010,BattyMorphetKiril2012} The highest entropy is then given by a pattern with a uniform distribution in geographical space as in pattern a) of figure \ref{fig:synthetic}. This interpretation answers the question: How uncertain is the absolute location of a place with a given characteristic?}

\item{The second basic phase space uses a characteristic of places or objects in space as the phase space.\cite{AgustGudmundsson2013} All patterns in figure \ref{fig:synthetic} have the same global proportions of black and white pixels and therefore, according to this phase space definition, the same entropy.
This interpretation answers the question: how uncertain is the characteristic of a given place or observed element in space in general, independent from the spatial configuration?}

\item{Most reviewed approaches to spatial entropy use a combination of the two phase spaces above. They are measures of spatial evenness widely discussed in the literature on measures of segregation \cite{Theil1971,duncan1955,GiniIndex,DIndex,EtaSquared,White1983,morgan1983,Logan,LiebersonCarter,MasseyDanton,WongSegregation,Taeuber1969}. They have the highest entropy if entropy is maximised in both phase spaces above at the same time. It answers the question: how evenly are observations of different types or characteristics distributed geographically?}
\end{itemize}




Entropy in an urban context also appears in Wilson's spatial interaction models\cite{Wilson1970}, and there are attempts to discuss the energy and resources entering and exiting an urban systems in releation to entropy.\cite{EcosystemEntropy} These approaches to entropy in cities are very remote from our approach and not discussed here.\\

Entropy across multiple scales has been applied to measure complexity in time series, for example by Zhang \cite{ZhangPhaseSpaceApproach} and Costa et al. \cite{costa2000multiscale,costa2002multiscale,costa2005multiscale,costa2015generalized} for medical time series. This ``has become a prevailing method to quantify the complexity of signals. It has been used successfully'' \cite{MultiscaleEntropyReview} in numerous fields of research, but to our knowledge not in a spatial context. In contrast to our approach, a single entropy value is produced from integrating over entropies after coarse graining at different scales. By adding the entropies, different scales are viewed as independent systems.\\ 
% In practice this would capture that for example pattern e) in \ref{fig:synthetic} has a wide variety of values across all neighbourhood sizes, but ignore that small low value neighbourhoods lie very predictably in large low value neighbourhoods and vice versa.



% We can draw four random places from patterns a) and b) and look at their own values and their neighbourhoods as in figure \ref{fig:samples}
% How much uncertainty is there about the value of the place itself and the mean value of its neighbourhood?
% In both patterns we are equally likely to draw a black or a white pixel. But in pattern a)  we can be certain that it will be in an ``even mix'' type of neighbourhood. In pattern g) on the other hand the type of neigbhourhood is much harder to predict.


% If we consider a binary pattern with N pixels, and distinguish all combinations precisely, there are $2^N$ possible patterns.

% This is consistent with recombining groups until reaching a group size of N pixels, where the total number of possible configurations C is given as the recursive function

% $$C(N) = C(N/2)^2 \quad \text{with} \quad C(1) = 2  $$

% Then $$ C(N)= 2^N$$


% The maximum entropy with $2^N$ states is $\log{\left(2^N\right)}$. There are $\log_2{N}$ steps of combining groups, and each steps adds an entropy of $\log{2}$. We can split the total entropy $\log{\left(2^N\right)}$ in the partial entropies added by each step of combining groups:

% $$\log{\left(2^N\right)} = \log_2{\left(N\right)} \log(2)$$

% Each step is the entropy at that step minus the entropy from the previous step:
% $$H(x)-H(x-1)= \log{n(x)} - \log{\frac{n(x)}{2}} = \log{2}$$

% This cancels out all entropies except for the last step. But we want to capture the amount of randomness at all scales of the spatial configuration. The multiscale entropy has then a maximum entropy of 
% $$\log(2)$$



\subsection*{The Multiscale Entropy Phase Space}
In contrast to the existing measures of entropy discussed above, we want to answer the question: how uncertain is what a randomly selected resident does, based on the structure of the city? For places in the city this would mean: how uncertain are the characteristics of places a person could be in, considering that the characteristics of a place are defined not only by its own value, but also by the characteristics of the places around it? In that sense, none of the approaches described above is a conceptually consistent interpretation of entropy that reflects the idea of cities as emergent phaenomena. Instead we need a measure that fulfills the following requirements:
\begin{itemize}
\item{It should observe how places are distributed across characteristics, to reflect the certainty about what people do in them.}
\item{It should reflect that the characteristics of places spatially depend on each other, because the surroundings of a place fundamentally alters how it can be used.}
\end{itemize}

%\subsection*{Spatial dependence}
In the following we demonstrate with a combinatorical example how spatial relationships can be taken into account by observering configurations at multiple scales.\\
As a thought experiment, we construct a pattern by arranging individual pixels into groups of two, then arranging these groups into groups of four and so on. We observe the randomness involved in each step of the process.\newline
Individual pixels can be white or black. The color of a random pixel can be very predictable (low entropy), for example if all pixels are black. Or it can be distributed in a less predictable, more random way, for example if half of the pixels are black and the other half white. So far this corresponds to the non spatial Shannon entropy, or group 2 in the common phase space definitions above.\newline
The next step takes into account that each pixel's characteristic should also depend on what kind of pixel is next to it. We form pairs of two pixels that will be arranged next to each other in the final pattern. this can be done in a very predictable way: entropy is low if all 2-pixel combinations are exactly the same, for example if every single white pixel is next to one black pixel and vice versa. They could instead be combined in a less predictable way: for example if one third of the combinations has two black pixels, one third has one white and one black pixel and one third has two black pixels. There is clearly less certainty and higher entropy in the spatial combinations of the second case.\\
These groups of two can then be combined into groups of four, again in more or less predictable ways and so on, until the pattern is complete.
If we make the least predictable, or, the maximum entropy choice in every step of building the spatial configuration, we must end up with a nontrivial pattern like the linear pattern in figure \ref{fig:linearpattern}.

\begin{figure}[ht]
   \centering
   \includegraphics[width=\textwidth]{./paper_sr_format/linearpattern.png}
   \caption{a linear pattern with equal probability to find any combination of 2 or 4 adjacent pixels (when divided into non overlapping segments)}
   \label{fig:linearpattern}
\end{figure}
Again: the phase space here is \emph{not} the geographical space. We associate high entropy \emph{not} with randomness in absolut positions in a pattern, but instead  with randomness in the \emph{spatial configurations}. \\

This makes sense intuitively if relate this back to cities. Imagine the patterns in figure \ref{fig:synthetic} were real cities, and black and white pixels would refer to residential and commercial buildings. Of course pattern a) is more evenly distributed in space, but this is not what we are interested in. Taking into account the surroundings of each pixel, pattern a) only has two different types of places: residential or commercial buildings, but always in mixed blocks in mixed neighbourhoods in mixed districts of a homogeneously mixed city. Pattern g) has a much larger variety of spatial configurations. If we pick a single place at random, we have the same probability to pick a ``residential'' or a ``commercial building'' in both cases, but in pattern g) there is much less certainty about the type of neighbourhood it is in. We want to extend the description used to compare individual buildings from ``a residential building'' to something like ``a residential building in a mixed use block which itself lies in a mainly commercial district that is surrounded by residential areas''. All of these surroundings at different scales should be part of the state of that place: if the direct surroundings and larger scale neighbourhoods of two places are identical, their function is more simmilar. In reverse, if two places are identical but their surroundings fundamentally different, they can be used in different ways and their states should differ. So what ``Multiple scales'' means is that the state of a place includes values describing not only the place's own characteristics, but some aggregate description of its environments within increasing distance: its immediate surroundings, its local neighbourhood and its larger scale environment. 

Therefore we define for our quantitative measure the phase space like this: the first dimension of the phase space is the value of a place's own characteristic. We then add further phase space dimensions describing the place's surroundings at $n$ different scales. When we consider only one characteristic, the state of each place $x_i$ in the city is given by the vector
\begin{equation}
\label{c_state_vector}	
\vec{x_i} = (x_i^{d_0}, x_i^{d_1}, \ldots, x_i^{d_n})
\end{equation} 
\bigskip

Where $x_i^{d_0}$ is the local value of characteristic of place $x_i$ itself, and $x_i^{d_n}$ is given by the local characteristics' values of all places within distance $d_n$ from $x_i$, aggregated by a function:
\begin{equation}
\label{multiscale_vector}
x_i^{d_n} = f(x_{k_1}^{d_0},x_{k_2}^{d_0}, \ldots, x_{k_m}^{d_0})
\end{equation}
for all $x_{k}^{d_0}$ of the $m$ places within distance $d_n$ from $x_i$. What this achieves is that we can distinguish between locally identical places based on what kind of area they are in, because the state of a place is literally a function of its surroundings. 

Extending this to $c$ scalar characteristics, the whole state of a place in the system is given by the matrix
\begin{equation}
\label{placestatematrix}
\Psi_i = \begin{pmatrix}
x_i^{d_0,1} & x_i^{d_1,1} & \ldots & x_i^{d_n,1}\cr
x_i^{d_0,2} & x_i^{d_1,2} & \ldots & x_i^{d_n,2}\cr
\vdots &\vdots & \vdots & \vdots \cr
x_i^{d_0,c} & x_i^{d_1,c} & \ldots & x_i^{d_n,c}\cr
\end{pmatrix}
\end{equation}

\bigskip

To exemplify how $\Psi$ adds the spatial context of a place to its state imagine a number of identical offices. One of them is in a central business district, one of them on the countryside, one on an oil platform and another in the dessert. The way they are or can be used is fundamentally different because the spatial context is. That context appears in $\Psi$. In reverse, the way the ocean / countryside / business district / dessert around them are used is altered as well, and the presence of the office building appears in their state matrices.

Theoretically, the continuous phase space $\phi^{(n+1)*k}$ that includes $c$ characteristics at $n$ scales is then the space of all possible place state matrices $x \in \phi$.
The states of all places in the system are distributed in $\phi$ with the probability density distribution
$g$. The multiscale entropy $H_{ms}$ is then 
\begin{equation}
\label{eq:theoretical_multiscale_phasespace}
H_{ms} = \int_{a \in \phi}{\! - g(a) \log{g(a)}} \, \mathrm{d}a
\end{equation}
for all possible states $a$ where $g(a)>0$.\\

There are strong conceptual parallels to the difference between Boltzman's and Gibbs' phase space in statistical mechanics. For $c$ characteristics observed at $n$ scales, the phase space has $c * n$ dimensions, signifcantly more than the $c$ dimensional space that could be used if we ignored interactions between places. A similar increase in the number of dimensions happens when moving from the Boltzman phase space to the Gibbs phase space that deals with interacting particles in statistical mechanics. Nonetheless, we are not moving moving to a Gibbs space, and the relationship between the Boltzmann entropy and the Gibbs entropy is far more complex \cite{GibbsVSBoltzmann}, and their precise interpretations still debated \cite{GibbsDiscussion2015}. 

\subsection*{Multiscale Entropy Estimation}

The practical entropy estimation in our simulated patterns and case study is as simple as possible without compromising the general concept. 
We use simplified square neighbourhoods with varying side length because it makes the results easy to trace, is computationally convenient and is sufficient to demonstrate the concept. \\
We then calculate the place state matrix $\Psi$ using the mean as the aggregation function in equation \ref{multiscale_vector} for the same reasons.\\

We discretise the phase space by defining a descrete set of values for all the elements in matrix $\Psi$ that will be given by bining the values after the aggregation. Places are assumed to have the same state if and only if their state matrices are exactly identical. Because this simplified phase space is discrete, we can estimate the probability of discrete states directly from their frequency and estimate the system's entropy directly with equation \ref{eq:shannon_entropy}. \\

Discretising the phase space has multiple advantages. First, we avoid properties of the unit dependent\cite{differentialEntropyCoordinateSystem} continuous entropy such as negative entropy\cite{negativeEntropy1,negativeEntropy2} that are difficult to interpret in terms of statistical mechanics. Furthermore, it removes the difficulty of evaluating eucledian distances between values of different place characteristics for equation \ref{eq:theoretical_multiscale_phasespace}. Finally, it allows us to avoid discussing complicated estimators for multivariate continuous data\cite{Voronoi,KdEntropy}. They are unreliable for high dimensional data because they work with the spaces between observations, and the number of data points on the edges of the phase space increases exponentially with increasing dimensions.\\

In the following chapter the multiscale entropy estimation method is applied to simulated patterns. 


\section*{Simulated Patterns}
Here we measure the entropy of synthetic spatial patterns with varying complexity (figure \ref{fig:synthetic}) according to the two essential phase space definitions in the literature, and according to our multiscale entropy measure. We first show how the results of our new method are inherently different from the non spatial phase space and the geographical phase space. We then compare the multiscale entropies of patterns with different structures and show that if interactions between places are accounted for, complex patterns have a higher entropy than simple ones.\\

We use the \Sexpr{length(synthetic_names)} artificial patterns from figure \ref{fig:synthetic}. The patterns are \Sexpr{syntheticsize} pixels wide and high. Each pixel corresponds to a "place". Each pixel is assigned a value from 0 to 1 (black to white), defining the only characteristic of that place. For "neighbourhoods" that go over the edge of the pattern, the invisible part is assumed to have the same proportion of values as the visible part. We bin the mean values in three categories: mainly low values (mean 0-0.33), mixed (mean 0.33-0.66) and mainly high values (mean 0.66-1.0). We use 5 different scales with neighbourhood side lengths with \Sexpr{paste(paste(lags[-length(lags)],collapse = ", "), "and", lags[length(lags)])} pixels.
%
\begin{figure}[ht]
   \centering
   \includegraphics[width=\textwidth]{./code/output/syntheticsynthetic_patterns.jpg}
   \caption{patterns a) - g)}
   \label{fig:synthetic}
\end{figure}
%

The patterns are selected to represent varying degrees of complexity. When we speak of ``complexity'' here, we mean patterns which could be described intuitively as having ``meaningful structural richness''.\cite{EnotComplex1}. Pattern a) has a spatially uniform probability for all pixels to be either white or black. It is fully random and arguably has no structure at all. Randomised checker boards b)-d), are essentially fully random patterns like pattern a), but pixels of each color appear in patches of increasing size. They are increasingly segregated and could be seen to be increasingly ordered with increasing patch size, but the order is very simple. Pattern e) is a sample from a uniform distribution between 0 and 1 (corresponding to black and white), sorted linearly from left to right and from top to bottom. There is structure, but no structural richness in that sense. Potentially viewed as slightly more complex might be pattern f), in which pixels are assigned a 1 or a 0 with the probability to find a black pixel decreasing linearly from left to right. It serves as the binary spatial counterpart to what Zhang\cite{ZhangPhaseSpaceApproach} considers a complex time series, but arguably does not differ greatly from pattern e) in terms of structural richness. Pattern g) results from a binarised additive cascade process, which produces patterns with multifractal self-similar properties stemming from ``complex processes'' \cite{cheng1999multifractality} that are regularily associetated with high complexity.\cite{cascadeComplex1,cascadeComplex2,cascadeComplex3}.
%
\subsection*{existing measures}
In the non spatial phase space observing only global characteristic proportions, we can directly tell that all patterns would display the same entropy as long as they differ only in the spatial configuration. If we consider only two states, values greater or smaller than $0.5$,  the pattern's entropy according to equation \ref{eq:shannon_entropy} is $H_{non spatial} = \log(2)$ because in all patterns approximately half the pixels have a value greater than $0.5$. We could reduce the multiscale entropy phase space to this by using only the $x_i^{d_0,c}$ column on the left of $\Psi$.\\

Measures of entropy using the geographical space directly as the phase space are essentially measures of how evenly elements are distributed across different zones. We split the patterns into square zones with a side length of \Sexpr{geospace_aggregationfactor} pixels, and count the number of black pixels as in figure \ref{fig:syntheticgeospace_zones}.\\ 
\begin{figure}[ht]
   \centering
    \includegraphics[width=\textwidth]{./code/output/syntheticgeospace_zones.jpg}
   \caption{probability to find black pixels in zones for the geographical phase space}
   \label{fig:syntheticgeospace_zones}
\end{figure}
This approach is inherently different from our measure in its goals and results. As expected from a measure of spatial evenness, the geographical phase space entropy (figure \ref{fig:syntheticgeospace_entropy}) is highest for the uniform distribution (figure \ref{fig:synthetic} a)), and lowest for patterns segregated spatially at a larger scale than the used zones (figure \ref{fig:synthetic} c) and d)).

\begin{figure}[ht]
   \centering
    \includegraphics[width=\textwidth]{./code/output/syntheticgeospace_entropy.jpg}
   \caption{Entropies for patterns in figure \ref{fig:synthetic} in the geographical phase space}
   \label{fig:syntheticgeospace_entropy}
\end{figure}


The frequencies in the discrete phase space in figure \ref{fig:syntheticgeospace_frequency} show the conceptual difference to our measure. When the geographical space is used directly as the phase space, the spatially even distribution of pattern a) also gives an even distribution in the phase space. In contrast, we see an \emph{even distribution of frequencies} for the sorted patterns e) and f) and for the additive cascade g), which are favoured by a measure that is focused on how much places differ from each other.

\begin{figure}[ht]
   \centering
    \includegraphics[width=\textwidth]{./code/output/syntheticgeospace_frequency.jpg}
   \caption{black pixel frequencies in the zones of the geographical phase space, patterns a)-g) in figure \ref{fig:synthetic}}
   \label{fig:syntheticgeospace_frequency}
\end{figure}


\subsection*{multiscale entropy and complexity}

The simulated patterns corresponding to the patterns in figure \ref{fig:synthetic} have the multiscale entropies shown in figure \ref{fig:synthetic_boxplot}.  


\begin{figure}[ht]

   \centering
   \includegraphics[width=\textwidth]{./code/output/syntheticboxplot.jpg}
   \caption{multiscale entropy for patterns a) - g)}
   \label{fig:synthetic_boxplot}
\end{figure}


To show how spatially complex patterns have the highest entropy if interactions between locations are taken into account, we take a closer look at the distributions in the multiscale phase space for the synthetic patterns. Figure \ref{fig:syntheticsynthetic_patterns_phasespace} shows two dimensions of the multiscale phase space, specifically at the scales of \Sexpr{lags[synth_phasespace_plot_scales[1]]} and \Sexpr{lags[synth_phasespace_plot_scales[2]]} pixels neighbourhood side length.

\begin{figure}[ht]

   \centering
   \includegraphics[width=\textwidth]{./code/output/syntheticsynthetic_patterns_phasespace.jpg}
   \caption{phase space distributions for patterns a) - g) at scales of \Sexpr{lags[synth_phasespace_plot_scales[1]]} and \Sexpr{lags[synth_phasespace_plot_scales[2]]} pixels neighbourhood side length}
   \label{fig:syntheticsynthetic_patterns_phasespace}
\end{figure}

The relatively complex additive cascade is most evenly distributed in the phase space. The uniform probability in the geographical space of pattern a) is distributed relatively evenly on the very local scale, because locally, we are likely to find all possible combinations of pixel colors. However, all pixels lie in very similar mixed neighbourhoods, and so the distribution has little variation in the larger neighbourhoods of the y axis. The patterns b) to d), segregated on different scales, have increased variance on scales of observation close to their scale of segregation, but fail to maintain variance across multiple scales. The sorted uniform distribution of pattern e) is very evenly distributed on all scales individually. However, there is no variation in which type of small scale neighbourhood is combined with which type of larger scale neighbourhood. This effect also applies to the 1/f noise pattern: While there is some variation on all scales, small white pixel neighbourhoods are systematically more likely to lie in larger white pixel neighbourhoods and vice versa.\

Imagine we would try to change any of these patterns to spread the observations more evenly in the phase space and increase the entropy. We would need to add more and more layers of variation on different scales, while simultaneously trying to avoid creating simple random noise, and the result would be a spatially complex configuration similar to the additive cascade.

This may seem rather abstract. However, it should apply to any system in which elements interact with and influence each other over multiple scales of some type of ``nearness'', to a degree at which they fundamentally change each others meaning. As discussed in the introduction this is certainly the case for places in cities. Under these circumstances, complex patterns have a higher entropy. Therefore, we can and should expect the whole system to eventually arrange in a complex pattern, simply because that is the most probable configuration. 


\section*{Case Study: London 1875 - 2005}
\subsection*{data}
\noindent In the case study, we analyse the spatial patterns of land use in west London from 1875 - 2005. The dataset used in the analysis was originally built and provided by Stanilov et al.\cite{stanilovData1}. It covers 200 square kilometers, spanning 20km from east to west, from London's green belt in the west to the west end hyde park, and roughly 10km from north to south. The data provides the land use of individual building in 32 categories for seven moments in time; 1875, 1895, 1915, 1935, 1960, 1985 and 2005.
For further details on the data collection see Stanilov et al.\cite{stanilovData1}

\subsection*{entropy estimation}
To keep the number of dimensions reasonably low, the 32 land uses are grouped into three categories of "business", "residential" and "leisure" and we use \Sexpr{length(lagsINmeters)} scales of observation at \Sexpr{paste(paste(lagsINmeters,"m",sep=""),collapse=", ")}. We discretise the values in the place state $\Psi$ equidistantly in three bins. The data is rasterized at a resolution of 50m. Neighbourhood parts outside the bounding box are assumed to have the same proportion as the parts within.\\

We compare the observed patterns with three null models that are constructed to preserve the global amount of different land uses and differ only in the spatial configuration. The configurations for comparison are shown in figure \ref{fig:real_vs_null_rasters}
\begin{itemize}
\item spatially random uniform spread: the pixels of the original data are reallocated in a random order. This would be the maximum entropy distribution if the phase space was directly taken from the geographical space. 
\item compact mixed-use growth: the pixels of the original data are redistributed in a fully random fashion, but separated between developed and undeveloped land and fit compactly to the east edge, corresponding to the general direction of growth in the original data. 
\item compact segregated growth: the pixels of the original are sorted by function and fit compactly to the east edge.
\end{itemize}

We also compute the non spatial entropy of the global proportion of functions for each year.

Further details on the data, preprocessing and the construction of the null models can be found in the supplementary material \ref{appendix}.

\subsection*{results}
Figure \ref{fig:real_vs_null_entropies}  shows the development of entropy over time in comparison to three null models and non spatial entropy.

For all cases, entropy generally increases until 1935, stagnates around 1965 and then slightly decreases until 2005. This is based on the non spatial entropy of the global distribution of functions, as almost the entire area is undeveloped in the beginning and almost filled entirely in the end.\\

The observed multiscale entropy of West London is significantly higher than all three null models. Especially between 1915 and 1960, entropy increases in the observed data, while the null models stagnate. \\

\noindent The grayscale images in figure \ref{fig:real_vs_null_rasters} show the probability of each pixel's state to investigate which places contribute to the total entropy. 

\noindent In the spatially uniform randomised case, unique places appear only beyond a certain global density, where only very small segregated clusters appear by chance. In the early stages entropy would be higher if growth was more concentrated, and later if there were also larger segregated and non segregated local concentrations.

\noindent In the compact mixed use growth case, the only unique places are on the city edge, while most places are either completely undeveloped or evenly mixed. Entropy could be increased by a less stringent city edge and partial concentration of the less frequent commercial functions.
In the compactly segregated case, the most unique places are along the edges between functions, as well as along the city edge. Entropy could be increased by a less stringent city edge, as well as more smaller clusters of segregated or mixed functions.

\noindent All of these alterations would change the null model patterns closer to what we actually observe:

\noindent First, clusters of different sizes with varying degree of functional segregation. Second, no strict city edge. In the language of urbanists, we could call this \emph{polycentricity}\cite{polycentricity} and \emph{sprawl}\cite{sprawl}. From this perspective, we can give an explanation of the polycentric sprawl that dominates the growth patterns of the observed area in terms of entropy: unless significant restrictions are in place, there are simply overwhelmingly more combinations of individual choices that lead to polycentric sprawl, making it the most likely pattern to occur. \\

There are great limitiations in terms of data and methodology that make any conclusions or generalisations speculative. First of all, we are only observing a small window of the city, and as the city grows the city edge passes through our field of view. Furthermore, the results may be biased towards higher entropy because in the original data collection, the area was selected specifically for it's high functional diversity.\cite{stanilovData1} \\
In terms of methodology the functional categories, the aggregation function, the scale of rasterisation, the selection of neighbourhood scales and their rectangular shape are all rather arbitrary. While sufficient to demonstrate the basic ideas, neighbourhood sizes and shapes as well as the aggregation function could use a network based measure of distance, take into account subjective travel cost and relate to insights into the actual connectivity between places. 

\begin{figure}[ht]
   \centering
    \includegraphics[width=\textwidth]{./code/output/real_vs_null_entropies.jpg}
   \caption{Multiscale entropy in West London over time compared to 3 null models}
   \label{fig:real_vs_null_entropies}
\end{figure}



\begin{figure}[ht]
   \centering
    \includegraphics[width=\textwidth]{./code/output/real_vs_null_rasters.jpg}
   \caption{The probability of each pixel's state, and the corresponding spatial distribution of functions. From left to right: random pixel allocation, compact mixed use growth, compact segregated growth and observed data. Global proportion of functions and observed data correspond to \Sexpr{paste(years,collapse=", ")} from top to bottom. Grey: undeveloped or no data. Red: residential. Blue: commercial. Black: leisure. Grayscale images decreasing probability with increasing brightness (logarithmic)}
   \label{fig:real_vs_null_rasters}
\end{figure}



\section*{discussion}


The ambition of this work is to make a contribution to explaining how individual actions shape cities and establish a more coherent relationship between entropy and complexity. Further, the general framework of thinking may be used as a strategy to deal with uncertainty and unpredictability in planning practice.

\emph{[...] ??? inconsistency because i moved parts to the introduction ...} The case study - that is arguably too small in scale and too simplified in its methodology to be generalisable in any way - suggest that West London did in fact display a higher entropy than the more extreme toy scenarios.\\

The understanding of the relationship between entropy and complexity is highly incoherent in the literature.\cite{Shalizi2004} Attempts have been made to associate complexity with decreasing thermodynamic entropy \cite{DecreasingEntropyIsComplex1,DecreasingEntropyIsComplex2}, regarding the occuring order as higher complexity than the original randomness. Others regard fully unpredictable signals such as white noise as ``fully complex''\cite{EisComplex1} in contrast to fully ordered signals such as strictly periodical signals. This view is also adopted by Batty et al. for the context of cities.\cite{BattyMorphetKiril2012} In contradiction, Costa et al. conclude that ``if one supposes that greater entropy is characteristic of greater complexity, such results are profoundly misleading.''\cite{costa2000multiscale}.\\

The point is that almost arbitrary results can be obtained depending on how the phase space is defined. The key is to define a phase space that is conceptually grounded in how the macroscopic state of the system is produced. We argue that in a system in which the microstates are spatially dependent, this must be considered. The analysis of synthetic patterns with multiscale spatial entropy shows that in that case, complex patterns have the highest entropy. We can thus partly explain the spatial complexity that is frequently observed in cities\cite{ScalingLaws,FractalCities,MultifractalBeijing,MultifractalZipf} - and more generally the complexity of patterns with interdependent observations -  as simply the kind of pattern we are most likely to observe because they can occur in more ways than others.\\

There are methodological and conceptual parallels of our approach to methods for estimating fractal dimensions \cite{mandelbrot}, specifically box counting \cite{boxcounting} which could be worth exploring further.




What is ignored so far entirely, except for a vague notion of some interaction between different places, is essentially everything else we already know about cities: how people use them, or how social and economic processes shape their structure. Paradoxically, that is precisely why this might be a powerful concept. It allows us to make \emph{the statistically best guess about what we do not know.} From a planners perspective, we would try to optimise our planning effort based on some assumptions about people and societies, how they should or want to use cities, and beyond that based on some prediction about the future and an assessment of what should be considered a ``good'' city. There is a limit to how certain we can be about these assumptions. If we believe to know a number of things with varying certainty, a conceptually consistent theory of urban entropy could be used to \emph{physically express that uncertainty in the structures we build.} That way we could increase the probability to have a positive result even if our assumptions were wrong. 

\newpage
\bibliography{master}
% \addcontentsline{toc}{chapter}{Bibliography}

\section*{supplementary material}
\label{appendix}


\subsection*{Data and Preprocessing}
Stanilov et al. \cite{stanilovData1} provide detailed information on the collection process of the data that involved digitising historical maps from Ordnance Survey (OS) on the scale 1:2,500. The land use classes were categorised manually: ``The process of land use classification involved the interpretation of building footprints from the OS maps; verification of building type (for buildings still in existence) in Microsoft Virtual Earth (now Bing Maps 2D and 3D) and Google Street View; and cross-referencing the results with several land use databases for Greater London'' \cite{stanilovData1}, while for some categories label information was given in the OS maps. It is evalutated to be a ``representative sample of Londons metropolitan fabric''\cite{stanilovData1}, but differences are pointed out between different parts of London on the scale of the study area. As in this study the diversity of land uses is itself at the focus of attention, the biased choice of the study area towards the section with the largest variety may propagate to make it less representative of London for this works purpose.\\

The functions in the original data are grouped into three main groups: residential, business and leisure (table \ref{tab:groups}). The chosen categories broadly reflect classifications used by geoinformation sciences \cite{categories1}, urban planning theory \cite{Jane} and practice \cite{categories2}, altered to cover a broader range of uses. ``Leisure'' is taken in the broadest sense of activities not related to workplace or home, including most categories not covered by the former two. A virtual fourth category appears in the data's empty space.

\begin{center}
\caption{Data Preprocessing: grouping function}
\begin{tabular}{|c|c||c|c||c|c|}
 \hline
\multicolumn{2}{|c||}{\textbf{Business}}&\multicolumn{2}{|c||}{\textbf{Residential}}&\multicolumn{2}{|c|}{\textbf{Leisure}}\\
 \hline
INS & institutional & APT & apartments & GEN & mixed/commercial\\
INSL & large institutional & APTH & high rise apt & RET & big box retail\\
O & office & COT & cottages & OLD & old fabric/mix\\
GAR & garages & DET & detached housing & AGR & alotment gardens \\
IND & industrial & DETH & high density detached & CEM & cemeteries \\
UTL & utilities & MEW & mews & EST & land estates \\
AIR & airport & SDT & semi-detached housing & FRM & farm structures \\
RRS & rail stations & TER & terraced housing & NRS & tree nurseries \\
  & & LDG & lodges / hotel & PRK & parks \\
  & & & & REC & recreational \\
  & & & & CHR & religious \\
  & & & & WAT & water \\
  & & & & STA & stadia \\
  & & & & SCH & schools \\
  & & & & CLR & cleared \\
 \hline
\end{tabular} 
\label{tab:groups}
\end{center}



All parts of the analysis are performed in R.\cite{R} The spatial data, provided in Shapefile format (.shp), is imported and transformed from the global positioning system (GPS) into the Universal Transverse Mercator coordinate system (UTM). It is then rasterized as a grid with at a resolution of \Sexpr{pixelWidth}m.


\subsection*{Method and Computation}

Then, the local values of all neighbourhood sizes are calculated and turned into a matrix in which there is a row for every point in space, and a column for the binned value of each category at each scale of observation. Identical rows are grouped, counted and translated to probabilities from which entropy is calculated.


The rasterised data is split into subsets containing each only one of the categories. We calculate the mean number of pixels of each category within a square moving window with the selected neighbourhood sizes:\Sexpr{paste(paste(lagsINmeters,"m",sep=""),collapse=", ")}. The choice of neighbourhood size requires further investigation in the future. For now, the number of scales is picked to keep a reasonable proportion between unique values for the place state matrix and the number of observed pixels. The largest size is selected to cover a substantial area of the total space, while still allowing for a sufficient number of non overlapping large scale areas. For neighbourhoods close to the edge, the missing neighbourhood area is assumed to have the same proportion of functions than in the available part. While this is preferable to a wrap-around torus, due to the asymetric nature of the data, it can lead to edge effects, making extreme values more likely along the edges. In a visual analysis of the probabilities of pixels in synthetic patterns and the study data, we found no considerable anomalies near the edges.\\

We now have a spatial matrix for each category at each scale of observation. The values are transformed into a single matrix with a row for each pixel, and column for each scale, giving a vector for each pixel with the total proportion of each category at each scale as a value between 0 and 1. The values are discretised in 3 equidistant bins, split at $1/3$ and $2/3$. The number of bins changes the total number of possible states and should return a reasonable proportion between possible states and observations. The binning is more than a technical question; One could ask how different two places need to be in their proportion of functions to have different states.
We then count how many vectors of each unique combination we find in the data and calculate the entropy from the probabilities directly.\\

We repeat the randomised null models as well as the synthetic patterns 200 times. The confidence intervals of the null models are too small to be visible in the plot because the number of pixels is a large sample within each randomised run already.

The synthetic patterns are produced as follows:\\

\textbf{pattern a) Uniformly random:}\\
All pixels are set to 0 or 1 with equal probability.\\

\textbf{pattern b)-d) Segregated:}\\
We start with a smaller matrix of which a side length for which the target size is a multiple of. All pixels are set to 0 or 1 with equal probability, and each pixel is expanded to multiple pixels so that the target size is produced. This process can lead to a varying number of 0's and 1's with increasing scale of segregation.\\

\textbf{pattern e) Sorted:}\\
A matrix is filled with a uniform distribution between 0 and 1 and sorted in x and y direction.\\

\textbf{pattern f) 1/f noise:}\\
A matrix is filled with 0's an 1's, with the probability to receive a 1 increasing linearly from 0 to 1 with increasing x position.\\

\textbf{pattern g) additive cascade:}\\
four different values are set and stored, their exact values are not relevant. A 2 by 2 matrix is created. Each pixel of the matrix is randomly set to one of the values. Each pixel is then expanded to 4 pixels, and randomly one of the four values is added to each of these. The process is repeated until the matrix target size is reached. Then, all values lower than the median are set to zero, all others to 1. 



\subsection*{sensitivity analysis}
NECESSARY?
-sensitivity to rasterisation resolution\\
-sensitivity to scales\\
-sensitivity to binning\\




















































































%
% \subsection*{Subsection}


% \begin{itemize}
% \item First item
% \item Second item
% \end{itemize}

% \subsubsection*{Third-level section}
 
% Topical subheadings are allowed.

% \section*{Discussion}

% The Discussion should be succinct and must not contain subheadings.

% \section*{Methods}

% Topical subheadings are allowed. Authors must ensure that their Methods section includes adequate experimental and characterization data necessary for others in the field to reproduce their work.

% \bibliography{sample}

% \noindent LaTeX formats citations and references automatically using the bibliography records in your .bib file, which you can edit via the project menu. Use the cite command for an inline citation, e.g.  \cite{Figueredo:2009dg}.

% \section*{Acknowledgements (not compulsory)}

% Acknowledgements should be brief, and should not include thanks to anonymous referees and editors, or effusive comments. Grant or contribution numbers may be acknowledged.

% \section*{Author contributions statement}

% Must include all authors, identified by initials, for example:
% A.A. conceived the experiment(s),  A.A. and B.A. conducted the experiment(s), C.A. and D.A. analysed the results.  All authors reviewed the manuscript. 

% \section*{Additional information}
% To include, in this order: \textbf{Accession codes} (where applicable); \textbf{Competing financial interests} (mandatory statement). 

% The corresponding author is responsible for submitting a \href{http://www.nature.com/srep/policies/index.html#competing}{competing financial interests statement} on behalf of all authors of the paper. This statement must be included in the submitted article file.

% The author(s) declare no competing financial interests.


% \begin{figure}[ht][ht]
% \centering
% \includegraphics[width=\linewidth]{stream}
% \caption{Legend (350 words max). Example legend text.}
% \label{fig:stream}
% \end{figure}

% \begin{table}[ht]
% \centering
% \begin{tabular}{|l|l|l|}
% \hline
% Condition & n & p \\
% \hline
% A & 5 & 0.1 \\
% \hline
% B & 10 & 0.01 \\
% \hline
% \end{tabular}
% \caption{\label{tab:example}Legend (350 words max). Example legend text.}
% \end{table}

% Figures and tables can be referenced in LaTeX using the ref command, e.g. Figure \ref{fig:stream} and Table \ref{tab:example}.


\end{document}